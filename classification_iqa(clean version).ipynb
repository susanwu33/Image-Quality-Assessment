{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "from PIL import Image\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import transforms, models, datasets\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\n#import cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport random\nfrom torchvision.models import resnet50, efficientnet_b0, inception_v3, efficientnet_b4\nfrom torchvision.models import ResNet50_Weights\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nimport seaborn as sns\nimport csv\nimport pandas as pd\n\nfrom pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\nfrom efficientnet_pytorch import EfficientNet",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "f43a2c83"
    },
    {
      "cell_type": "code",
      "source": "# define the device (gpu/cpu)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "d9c1b8e0-07a2-4152-bfe5-1f973d98710a"
    },
    {
      "cell_type": "markdown",
      "source": "## Data Preparation",
      "metadata": {},
      "id": "9a8f55eb-710c-4ee8-b8e0-4cd44af7969b"
    },
    {
      "cell_type": "code",
      "source": "# folder path\ndir_paths = [r'SNACGH_Grade1_Disc', r'test-images', r'train-images', r'validation-images']\n\n# list to store files\nres = []\n\n# all the paths\nwhole_paths = []\n\n# image data array\nimages = []\n\n# image labels\nlabels = []\n\n# batch size and epochs\nbatch = 16\nepochs = 800\n\n# Iterate directory to store the paths\nfor dir_path in dir_paths:\n    for path in os.listdir(dir_path):\n        if path[0] != '.':\n            #print(path)\n            img_p = os.path.join(dir_path, path)\n            # check if current path is a file\n            if os.path.isfile(img_p):\n                #print(path)\n                # store the path names\n                whole_paths.append(img_p)\n",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "4681b64e-918a-4b4b-be5e-8e694b8c62ad"
    },
    {
      "cell_type": "code",
      "source": "# =============================================================================\n# Resize and Crop (use early)\n# =============================================================================\nclass ResizeCrop(object):\n    ''' Resize all to same size, then crop to get a square '''\n\n    def __init__(self, size):\n        self.resize = transforms.Resize(size=size, interpolation=transforms.InterpolationMode.NEAREST_EXACT)\n        self.centercrop = transforms.CenterCrop(size=size)\n\n    def __call__(self, sample):\n        image = sample['image']\n        \n        # resize and crop to same size for image\n        image = self.centercrop(image)\n        sample['image'] = self.resize(image)\n\n        return sample\n\n# =============================================================================\n# Horizontal Flip (all to one side)\n# =============================================================================\nclass RandomHorizontalFlip(object):\n    ''' Flips image horizontal when necessary (all to LE) --> if RE flip, if LE no flip\n    '''\n    \n    def __call__(self, sample):\n        image, filename = sample['image'], sample['filename']\n        tmp = filename.split('.')\n        detail = tmp[0].split('-')\n        \n        if detail[-1] == 'RE':   \n            image = transforms.functional.hflip(image)\n\n        sample['image'] = image\n        return sample\n    \n\n# =============================================================================\n# Random Vertical Flip (use early)\n# =============================================================================\nclass RandomVerticalFlip(object):\n    ''' Flips image vartically when random value higher than p\n    '''\n    def __init__(self, p=0.5):\n        self.p = p\n    \n    def __call__(self, sample):\n        image = sample['image']\n        \n        if random.random() < self.p:   \n            image = torchvision.transforms.functional.vflip(image)\n\n        sample['image'] = image\n        return sample\n    \n\n#\n# =============================================================================\n#  Normalise (use middle)\n# =============================================================================\nclass Normalize(object):\n    # used for both train and validation\n    def __init__(self, mean, std):\n        self.normalize = transforms.Normalize(mean, std)\n\n    def __call__(self, sample):\n        \n        image = sample['image']\n        sample['image'] = self.normalize(image)\n        \n        return sample\n\n\n# =============================================================================\n# ToTensor class (use last)\n# =============================================================================\nclass ToTensor(object):\n    ''' Creates a tensor from an image '''\n    \n    def __call__(self, sample):\n        \n        sample['image'] = transforms.functional.to_tensor(sample['image'])\n            \n        return sample\n",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "da890f23-0dab-45a8-856e-c1cf080a758f"
    },
    {
      "cell_type": "code",
      "source": "# define the customized dataset\nclass MyDataset(Dataset):\n    \"\"\"my own dataset.\"\"\"\n\n    def __init__(self, paths, mode):\n        self.mode = mode\n        self.size = 380\n        \n        # define the transform\n        if mode == 'train':\n            self.transform = transforms.Compose([\n                ToTensor(),\n                ResizeCrop(self.size),\n                RandomHorizontalFlip(),\n                RandomVerticalFlip(p=0.5),\n                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n                Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n        else:\n            self.transform = transforms.Compose([\n                ToTensor(),\n                ResizeCrop(self.size),\n                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n                Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n            \n        '''\n        # split the sets\n        SEED = 62\n        random.seed(SEED)\n        random.shuffle(paths)\n        \n        if mode == 'train':\n            self.paths = paths[:int(0.7*len(paths))]\n        elif mode == 'test':\n            self.paths = paths[int(0.7*len(paths)):int(0.85*len(paths))]\n        else:\n            self.paths = paths[int(0.85*len(paths)):]\n        '''\n            \n        self.paths = paths\n                    \n        \n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        im = Image.open(self.paths[idx]).convert(\"RGB\")\n        #print(im.size)\n        #print(self.paths[idx])\n        \n        '''\n        width, height= im.size\n        # Setting the points for cropped image\n        left = 12\n        top = 12\n        right = width-12\n        bottom = height-12\n        im = im.crop((left, top, right, bottom))\n        #print(im.size)\n        '''\n        \n        # store the labels\n        path = os.path.basename(self.paths[idx])\n        tmp = path.split('.')\n        filename = tmp[0].split('-')\n        ori_label = int(filename[0])\n        \n        # for original dataset\n        if ori_label > 2:\n            label = 1\n        else:\n            label = 0\n        #print(label)\n        \n        '''\n        # for testing dataset\n        if ori_label == 1:\n            label = 1\n        else:\n            label = 0\n        #print(label)\n        '''\n        \n\n        item = {\n            'image': im,\n            'label': label,\n            'filename': path  \n        }\n        \n        \n        if self.transform:\n            item = self.transform(item)\n        \n        #print(im.shape)\n\n        return item\n    \n    \n    \n    def load_gradcam_item(self, idx):\n        \n        im = Image.open(self.paths[idx]).convert(\"RGB\")\n        input_tensor, label = self.__getitem__(idx)['image'], self.__getitem__(idx)['label']\n        \n        # store the filename (including .png)\n        filename = os.path.basename(self.paths[idx])\n        \n        return {\n            'image':im,\n            'tensor':input_tensor,\n            'label':label,\n            'filename':filename\n            }\n\n",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "e5b2ca13-015c-4eac-a786-29197033ac5c"
    },
    {
      "cell_type": "code",
      "source": "    \ntrain_ds = MyDataset(whole_paths, 'train')\ntest_ds = MyDataset(whole_paths, 'test')\nval_ds = MyDataset(whole_paths, 'val')\n\n#print(train_ds.__len__())\n#print(val_ds.__len__())\n    \n    \ntrain_loader = DataLoader(train_ds, batch_size=batch, drop_last=True, shuffle=True) # train\ntest_loader = DataLoader(test_ds, batch_size=2, shuffle=False) # test\nval_loader = DataLoader(val_ds, batch_size=2, shuffle=False) # val\n\n\nprint(\"LOADERS: train, test, validation:\", train_loader, test_loader, val_loader)\n",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "a55fa2db-42b5-4748-bdd4-714050938328"
    },
    {
      "cell_type": "code",
      "source": "# test images\n\ntest_img_dir = r'E:\\E-GLAU ONH Cirrus En Face Images'\n\ntest_img_whole_paths = []\n\nfor path in os.listdir(test_img_dir):\n    if path[0] != '.':\n        #print(path)\n        img_p = os.path.join(test_img_dir, path)\n        # check if current path is a file\n        if os.path.isfile(img_p):\n            #print(path)\n            # store the path names\n            test_img_whole_paths.append(img_p)",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "f835aea4-344c-4396-a504-b4b3e44ef383"
    },
    {
      "cell_type": "code",
      "source": "new_test_ds = MyDataset(test_img_whole_paths, 'test')\nnew_test_loader = DataLoader(new_test_ds, batch_size=2, shuffle=False) # test",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "28c5f030-9ab6-4a2b-be4c-bbc0037a2c7b"
    },
    {
      "cell_type": "code",
      "source": "\n# image, label = ....\n# plot the image (hint: .detach().cpu().numpy() and .permute() \n# print the label \nif False:\n    img, label = next(iter(train_loader))\n\n    print(img)\n    print(label)\n    plt.imshow(np.transpose(img[0].cpu().detach().numpy(), (1, 2, 0)))\n    plt.title(f\"Label: {label[0]}\")",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "9a53f468-3965-4e38-be22-db595d862742"
    },
    {
      "cell_type": "markdown",
      "source": "## Model Defination and Training Process",
      "metadata": {},
      "id": "bbe05e08-f96e-4b45-8559-13367ea02787"
    },
    {
      "cell_type": "code",
      "source": "def save_checkpoint(dir_name, epoch, f1, model):\n    \n    # If folder doesn't exists, create it #\n    if not os.path.isdir(dir_name):\n        os.mkdir(dir_name)\n        \n    filename = \"checkpoint-{:.3f}-{}.pth\".format(f1, epoch)\n    whole_name = os.path.join(dir_name, filename)\n    torch.save(model.state_dict(), whole_name)\n",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "2d51f531-46d8-44f5-b07c-0ff96f2504b8"
    },
    {
      "cell_type": "code",
      "source": "# models\n\n# ResNet50 (here we just simple use the predefined model, only change the last fc layer to fit the problem)\n#model_ft = resnet50(pretrained=True) # for python 3.6\n#model_ft = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)  #for python 3.9  #Alternatively, we can use weights='DEFAULT'\n#num_ftrs = model_ft.fc.in_features\n# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n#model_ft.fc = nn.Linear(num_ftrs, 4)\n#model_ft = EfficientNet.from_pretrained('efficientnet-b0', num_classes=4)\n\n#efficientnet\nmodel_ft = efficientnet_b0(weights='DEFAULT')\nin_ftrs = model_ft.classifier[1].in_features\nmodel_ft.classifier[1] = nn.Linear(in_ftrs, 2)\n\n# inception v3\n#model_ft = inception_v3(weights='DEFAULT')\n#in_ftrs = model_ft.fc.in_features\n#model_ft.fc = nn.Linear(in_ftrs, 4)\nprint(model_ft)",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "d2df1888-0649-4ca3-8adc-9ebcd2e2695a"
    },
    {
      "cell_type": "code",
      "source": "model_ft = model_ft.to(device)",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "a38eeea2-2675-4aaf-adce-83e7dc568e76"
    },
    {
      "cell_type": "code",
      "source": "# loss functions\nclass_weight = torch.tensor([0.4, 0.6]).to(device) # balance the class\n#criterion = nn.CrossEntropyLoss()\ncriterions = {'ce': nn.CrossEntropyLoss(weight = class_weight)}",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "36e32fd3-c792-4d24-95fd-43f0465f63c1"
    },
    {
      "cell_type": "code",
      "source": "\ndef train_model(train_data, val_data, model, epochs, dir_name):\n    \n    model = model.to(device)\n    # define the optimization\n    # Observe that all parameters are being optimized\n    optimizers = {'sgd': optim.SGD(model.parameters(), lr=0.001), 'adam': optim.Adam(model.parameters(), lr=0.001)}\n    # Decay LR by a factor of 0.1 every 7 epochs\n    lrsdls = {'sgd': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizers['sgd'], T_0 = 200, eta_min = 0.00001), \n             'adam': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizers['adam'], T_0 = 200, eta_min = 0.00001)}\n    #exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n    dataset_size = len(train_data.dataset)\n    val_dataset_size = len(val_data.dataset)\n    #print(dataset_size)\n    #print(val_dataset_size)\n    \n    # default\n    criterion = criterions['ce']\n    optimizer_ft = optimizers['sgd']\n    lrsdl = lrsdls['sgd']\n    \n    \n    # store the accuracy and loss\n    train_losses = []\n    train_accs = []\n    train_f1s = []\n    train_recalls = []\n    train_precisions = []\n    val_losses = []\n    val_accs = []\n    val_f1s = []\n    val_recalls = []\n    val_precisions = []\n    \n    f1_improve = 0 #check early stopping\n    \n\n    # enumerate epochs\n    for epoch in range(epochs):\n        \n        # choose the criterion\n        #if epoch >= 30:\n            #if epoch%20 == 0:\n                #criterion = criterions['ce']\n            #elif epoch%10 == 0:\n                #criterion = criterions['cdw']\n            #optimizer_ft = optimizers['adam']\n            #lrsdl = lrsdls['adam']\n        \n        # record the labels and predictions of each epoch\n        train_preds = []\n        train_labels = []\n        val_preds = []\n        val_labels = []\n        \n        running_loss = []\n        running_corrects = 0\n        model.train()\n        \n        # enumerate mini batches\n        for item in tqdm(train_data):\n            \n            train_labels.extend(item['label'].numpy())\n            \n            # pass the data to the device\n            inputs = item['image'].to(device)\n            targets = item['label'].to(device)\n            \n            # clear the gradients\n            optimizer_ft.zero_grad()\n            # compute the model output\n            \n            outputs = model(inputs)\n            #print(\"train\", outputs)\n            \n            _, preds = torch.max(outputs, 1)\n            train_preds.extend(preds.detach().cpu().numpy())\n            \n            # calculate loss\n            loss = criterion(outputs, targets)\n            \n            # credit assignment\n            loss.backward()\n            \n            # update model weights\n            optimizer_ft.step()\n            lrsdl.step()\n            \n            # statistics\n            running_loss.append(loss.detach().cpu().numpy())\n            running_corrects += torch.sum(preds == targets.data)\n            \n        epoch_loss = np.mean(running_loss)\n        epoch_acc = running_corrects / dataset_size\n        \n        epoch_loss = epoch_loss # .cpu()\n        epoch_acc = epoch_acc.cpu()\n        \n        train_losses.append(epoch_loss)\n        train_accs.append(epoch_acc)\n        \n        #print(train_labels)\n        #print(train_preds)\n        \n        '''\n        # manually compute the confusion matrix and some statistics\n        train_cf_matrix = [[0, 0, 0] for i in range(3)]\n        #rint(train_labels)\n        #print(train_preds)\n        for i in range(len(train_labels)):\n            # row true, column predicted\n            #print(train_labels[i])\n            train_cf_matrix[int(train_labels[i])][int(train_preds[i])] += 1\n        train_cf_matrix = np.array(train_cf_matrix)\n            \n        # precision i\n        tmp = np.sum(train_cf_matrix, axis=0)\n        train_precision_compute = np.array([train_cf_matrix[i][i]/tmp[i] for i in range(3)])\n        # recall i\n        tmp = np.sum(train_cf_matrix, axis=1)\n        train_recall_compute = np.array([train_cf_matrix[i][i]/tmp[i] for i in range(3)])\n        # f1 i\n        train_f1_compute = np.array([(2*train_precision_compute[i]*train_recall_compute[i])/(train_precision_compute[i]+train_recall_compute[i]) for i in range(3)])\n        '''\n        \n        \n        train_f1 = f1_score(train_labels, train_preds, average='macro')\n        train_recall = recall_score(train_labels, train_preds, average='macro')\n        train_precision = precision_score(train_labels, train_preds, average='macro')\n        \n        train_f1s.append(train_f1)\n        train_recalls.append(train_recall)\n        train_precisions.append(train_precision)\n\n        print('Epoch {} Training Loss: {:.4f} Acc: {:.4f} F1: {:.4f} Recall: {:.4f} Precision: {:.4f}'.format(epoch+1, epoch_loss, epoch_acc, train_f1, train_recall, train_precision))\n        \n        # validation\n        model.eval()\n        with torch.no_grad():\n            val_running_loss = []\n            val_running_corrects = 0\n            \n            for item in tqdm(val_data):\n                \n                val_labels.extend(item['label'].numpy())\n                val_inputs, val_targets = item['image'].to(device), item['label'].to(device)\n                val_outputs = model(val_inputs)\n                #print(\"val:\", val_outputs)\n                \n                val_loss = criterion(val_outputs, val_targets)\n                \n                _, val_pred = torch.max(val_outputs, 1)\n                val_preds.extend(val_pred.detach().cpu().numpy())\n                \n                # statistics\n                val_running_loss.append(val_loss.detach().cpu().numpy())\n                val_running_corrects += torch.sum(val_pred == val_targets.data)\n            \n            val_epoch_loss = np.mean(val_running_loss)\n            val_epoch_acc = val_running_corrects / val_dataset_size    \n            \n            #val_epoch_loss = val_epoch_loss.cpu()\n            val_epoch_acc = val_epoch_acc.cpu()\n            \n            val_losses.append(val_epoch_loss)\n            val_accs.append(val_epoch_acc)\n            \n            val_f1 = f1_score(val_labels, val_preds, average='macro')\n            val_recall = recall_score(val_labels, val_preds, average='macro')\n            val_precision = precision_score(val_labels, val_preds, average='macro')\n            \n            val_f1s.append(val_f1)\n            val_recalls.append(val_recall)\n            val_precisions.append(val_precision)\n            \n            print('Epoch {} Validation Loss: {:.4f} Acc: {:.4f} F1: {:.4f} Recall: {:.4f} Precision: {:.4f}'.format(epoch+1, val_epoch_loss, val_epoch_acc, val_f1, val_recall, val_precision))\n            \n            if val_f1 > 0.85:\n                f1_improve += 1\n                if val_f1 == max(val_f1s) and val_f1 > max(val_f1s[:-1]):\n                    save_checkpoint(dir_name, epoch+1, val_f1, model)\n                    f1_improve = 0\n                if f1_improve > 30:\n                    print('Epoch {} early stopping'.format(epoch+1))\n                    break\n                \n            \n    # plot\n    plt.plot(train_losses, label='Train')\n    plt.plot(val_losses, label=\"Validation\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title(\"Loss Graph\")\n    plt.show()\n    \n    plt.plot(train_accs, label='Train')\n    plt.plot(val_accs, label='Validation')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy Graph')\n    plt.show()\n    \n    plt.plot(train_f1s, label='Train')\n    plt.plot(val_f1s, label=\"Validation\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Macro F1')\n    plt.title(\"Macro-F1 Graph\")\n    plt.show()\n    \n    plt.plot(train_recalls, label='Train')\n    plt.plot(val_recalls, label=\"Validation\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Macro Recall')\n    plt.title(\"Macro-Recall Graph\")\n    plt.show()\n    \n    plt.plot(train_precisions, label='Train')\n    plt.plot(val_precisions, label=\"Validation\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Macro Precision')\n    plt.title(\"Macro-Precision Graph\")\n    plt.show()\n    \n    # confusion matrix of the last time\n    val_cfmtx = confusion_matrix(val_labels, val_preds)\n    #print(val_cfmtx.type)\n    val_cfmtxn = val_cfmtx.astype('float') / val_cfmtx.sum(axis=1)[:, np.newaxis]\n    sns.heatmap(val_cfmtxn, annot=True, fmt='.2%', cmap='Blues')",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "2f144883-32f0-478c-9486-3a6035ef6cff"
    },
    {
      "cell_type": "code",
      "source": "#model_ft.load_state_dict(torch.load('model_saved.pth'))\n        \n# train\ncheckpoint_dir_name = 'binary_classification_efficientnetb0_checkpoints'\ntrain_model(train_loader, val_loader, model_ft, epochs, checkpoint_dir_name)\n\n\n# save the model (only the weights)\ntorch.save(model_ft.state_dict(), 'model_saved.pth')",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "89843f25-4c00-45ff-b713-d162481bb687"
    },
    {
      "cell_type": "markdown",
      "source": "## Result Analysis and Visualization",
      "metadata": {},
      "id": "dad684f1-05ef-41ea-ae7b-d7a277dea88e"
    },
    {
      "cell_type": "code",
      "source": "load_model_path = r'C:\\Users\\user\\Downloads\\Susan_project\\binary_classification_efficientnetb0_checkpoints\\checkpoint-0.950-228.pth'\nmodel_ft.load_state_dict(torch.load(load_model_path))",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "8b76027e-67a8-4e35-886f-1333cfbd0a87"
    },
    {
      "cell_type": "code",
      "source": "\n#load the model again\n#model = TheModelClass(*args, **kwargs) // use the original model structure\n#model.load_state_dict(torch.load(PATH))\n#model.eval()\n\n# make prediction\ndef make_predictions(model, test_data):\n    model.eval()\n    test_labels = []\n    test_preds = []\n    #criterion = nn.CrossEntropyLoss()\n    \n    with torch.no_grad():\n        \n        for item in tqdm(test_data):\n            \n            test_labels.extend(item['label'].numpy())\n            test_inputs, test_targets = item['image'].to(device), item['label'].to(device)\n            test_outputs = model(test_inputs)\n            #print(\"test:\", test_outputs)\n                \n            test_loss = criterions['ce'](test_outputs, test_targets)\n                \n            _, test_pred = torch.max(test_outputs, 1)\n            test_preds.extend(test_pred.detach().cpu().numpy())\n    \n    return {\n        'groundtruth':test_labels,\n        'predictions':test_preds\n    }\n\n",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "88bfe714-ce7a-479c-af1e-e5d71b232b74"
    },
    {
      "cell_type": "code",
      "source": "\n# create a csv file to record the wrongly predicted ones\ndef load_error_record(ds, predicts):\n    with open('error_record.csv', 'w', newline='') as csvfile:\n    \n        fieldnames = ['filename', 'true_label', 'predicted_label']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n    \n        for i in range(len(ds)):\n            if predicts['groundtruth'][i] != predicts['predictions'][i]:\n                writer.writerow({'filename': ds.load_gradcam_item(i)['filename'], 'true_label':predicts['groundtruth'][i], 'predicted_label': predicts['predictions'][i]})\n\n",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "d65f1d9a-f05b-47a9-8ecd-951e42a9eb62"
    },
    {
      "cell_type": "code",
      "source": "#grad cam\ndef show_gradcam(model, target_layers, desired_ds, predictions, use_cuda=True, true_labels=True):\n    '''\n    Parameters\n    ----------\n    model : \n        The model to be used.\n    target_layers : \n        target layers... for resnet50 it's layer4\n    use_cuda : Boolean, optional\n        Whether to use cuda. The default is True.\n    desired_ds : MyDataset class\n        Which dataset want to use.\n    true_labels : Boolean, optional\n        Whether compare to true label. The default is False.\n\n    Returns\n    -------\n    None.\n\n    '''\n    # can obtain RBG img and tensor and label from Mydataset function load_gradcam_item\n    # take validation as example\n    with GradCAM(model=model, target_layers=target_layers, use_cuda=use_cuda) as cam:\n        \n        # save it to a new folder\n        dire = \"gradcam_figures\"\n        # If folder doesn't exists, create it ##\n        if not os.path.isdir(dire):\n            os.mkdir(dire)\n        \n    \n        for i in range(len(desired_ds)):\n            if true_labels:\n                targets = [ClassifierOutputTarget(category) for category in np.array([desired_ds.load_gradcam_item(i)['label']])]\n                grayscale_cam = cam(input_tensor=desired_ds.load_gradcam_item(i)['tensor'].unsqueeze(0), targets=targets)\n            else: \n                preds = [ClassifierOutputTarget(category) for category in np.array([predictions['predictions'][i]])]\n                grayscale_cam = cam(input_tensor=desired_ds.load_gradcam_item(i)['tensor'].unsqueeze(0), targets=preds)\n            \n            grayscale_cam = grayscale_cam[0, :]\n            img = transforms.CenterCrop(380)(desired_ds.load_gradcam_item(i)['image'])\n            img = transforms.Resize(380)(img)\n            img = (img - np.min(img)) / (np.max(img) - np.min(img))\n            visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n            pImg = Image.fromarray(visualization, 'RGB')\n            #break    \n    \n    \n            # adjust the filename\n            fn = desired_ds.load_gradcam_item(i)['filename']\n            if true_labels:\n                new_fn = ''.join(['true-', fn])\n            else:\n                # need the predictions to form the figures\n                fn = ''.join([str(predictions['predictions'][i]), fn[1:]])\n                new_fn = ''.join(['predicted-', fn])\n            #print(new_fn)\n            new_fn = os.path.join(dire, new_fn)\n            #print(new_fn)\n            #pImg.save(new_fn)\n            \n            if desired_ds.load_gradcam_item(i)['label'] != predictions['predictions'][i]:\n                #print('Mismatch!',desired_ds.load_gradcam_item(i)['label'], predictions['predictions'][i])\n                pImg.save(new_fn)\n            #else:\n                #print('Correct!',desired_ds.load_gradcam_item(i)['label'], predictions['predictions'][i])\n            #break\n            \n    load_error_record(desired_ds, predictions)\n    ",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "7756bacb-81cb-44a2-9686-95c10aa1a730"
    },
    {
      "cell_type": "code",
      "source": "new_predictions = make_predictions(model_ft, new_test_loader)",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "0fea6840-04b6-43f0-9ff5-d395839b5f21"
    },
    {
      "cell_type": "code",
      "source": "target_layers = [model_ft.features[-1]]",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "0ea9349c-4096-4b8e-b2a7-328ac08ea60a"
    },
    {
      "cell_type": "code",
      "source": "show_gradcam(model_ft, target_layers, new_test_ds, new_predictions, true_labels=False) ",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "2aa74726-9e7d-44e3-9868-0c991a668a8d"
    },
    {
      "cell_type": "code",
      "source": "predictions = make_predictions(model_ft, test_loader)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "38e01d1a-feab-4755-b443-44bcdb8e336d"
    },
    {
      "cell_type": "code",
      "source": "#target_layers = [model_ft.layer4[-1]]\ntarget_layers = [model_ft.features[-1]]",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "e9746687-082f-4c92-8253-27580e7ce991"
    },
    {
      "cell_type": "code",
      "source": "show_gradcam(model_ft, target_layers, test_ds, predictions, true_labels=True) # use true labels",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "17f8dbf3-ddd0-453b-a325-860c4f8b9ee9"
    },
    {
      "cell_type": "code",
      "source": "test_f1 = f1_score(predictions['groundtruth'], predictions['predictions'], average='macro')\ntest_recall = recall_score(predictions['groundtruth'], predictions['predictions'], average='macro')\ntest_precision = precision_score(predictions['groundtruth'], predictions['predictions'], average='macro')\nprint('Test: F1, Recall, Precision:', test_f1, test_recall, test_precision)\n\n# confusion matrix of the last time\n#test_cfmtx = confusion_matrix(predictions['groundtruth'], predictions['predictions'])\n#sns.heatmap(test_cfmtx/np.sum(test_cfmtx), annot=True, fmt='.2%', cmap='Blues')\n\n\n# now it can successfully save!\n# what next: save it as the original name and the predicted labels...?/\n# how to get the original filename?\n# how to get their predicted labels? -> put inside the make_predictions function",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": [],
      "id": "224e2f5e-3c7b-460c-ad3e-6bda327285d6"
    }
  ]
}